<!DOCTYPE html>
<html>
<head>
    <title>DATS 6501 Capstone Project</title>   
    <link rel="stylesheet" href="style.css"> 
</head>
<body>
    <nav>
        <ul>
            <li><a href="index.html">Home</a></li>
            <li><a href="problem statement.html">Problem Statement</a></li>
            <li><a href="procedure.html">Procedure</a></li>
            <li><a href="results.html">Results</a></li>
            <li><a href="analysis.html"  style="border-style: solid;">Analysis</a></li>
            <li><a href="conclusion.html">Conclusion</a></li>

        </ul>
    </nav>
    <h1>Analysis</h1>
    <p class="analysis">During the exploratory data analysis, there was no clear correlation between the occupation of the celebrity and the makeup of the text. 
        Celebrities’ texts did not share similar sentences in terms of the mean and standard deviation of the sentence lengths compared to texts of celebrities in the same industry. 
        For example, figures like Joe Biden, Elizabeth Warren, and Donald Trump, who are all prominent figures in US politics, have sentences of contrasting lengths and variation. 
        This suggests that occupation or other external variables do not need attention when attempting to personalize text.<br><br>
        In the data modeling phase, the text generated from the LSTM models built from scratch yielded underwhelming results and, as a result, highlights the magnitude of the challenge that comes with text generation. 
        For all personalities, many sentences were incoherent and sometimes even incomprehensible. A typical string of generated text consists of multiple typos and grammatical errors. 
        Even if the text is decipherable, it can not qualify as material that can be used in any type of AI software. 
        Some models generated text that were completely indecipherable, generating a seemingly random sequence of characters.
        Common phrases such as “thank you” and “I love you” were generated accurately for the twitter accounts that frequently used those phrases, but other less popular phrases were not reproduced by the model adequately. 
        <br><br>
        The GPT-2 model, on the other hand, had a more promising outcome. It generated text that, on surface level, appeared to resemble the text written by the celebrities. 
        From a human standpoint, the sentences were coherent, the word choices of both texts were comparable, and the emotional content from both texts were similar. On occasion, it did generate absurd sentences. 
        For instance, in a few cases, words were repeated multiple times in a sentence. 
        <br><br>
        The GPT-2 model’s generated text delivered mixed results when evaluated with quantifiable metrics. Firstly, the confidence intervals of the sentence lengths of the original and generated text overlapped for 16 out of the 30 personalities, meaning that only half of the celebrity’s tweets were properly generated in terms of sentence length. 
        Next, out of the 30 generated texts for the personalities, 15 yielded sentiment scores that were similar to the original (i.e. had a difference of less than 0.1).  
        Unfortunately, only 10 out of 30 had similar subjectivity scores compared to the original (i.e. having a difference of less than 0.1), suggesting that the GPT-2 model does not properly capture the subjectivity displayed in the celebrities’ text. 
        <br><br>
        Finally, although different Twitter accounts yielded different results based on the evaluation metrics, they performed similarly in terms of diction and word choice. 
        The word clouds for most of the original text corpuses are dominated by 2-3 words in large font, with a range of other words taking up a much smaller space in the diagram. 
        However, for the generated text, the words that dominate the word clouds rarely match those of the original text. 
        The word clouds illustrate that although the sentences generated by the GPT-2 model appear to be similar to the original text, they do not perfectly encapsulate the personalities’ language and vocabulary. 

    </p>

    
</body>
</html>